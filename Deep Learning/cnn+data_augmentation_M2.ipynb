{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn+data augmentation M2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-AxCPO5rQTF"
      },
      "source": [
        "make a convolutional neuronal netword for predicting if an image is cat or dog."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3v4dL602pUJ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNNO1TSVxmvL"
      },
      "source": [
        "original_dir = 'all/images'\n",
        "# le format: cat.i.jpg et dog.i.jpg, i un entier\n",
        "# Dans ce traivail on utilise pas l'ensemble des images, on essaie de voir\n",
        "# l'effet d'over-fitting quand on a pas assez de données"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epjJUaEiqg-a"
      },
      "source": [
        "# Créer le train_dir, val_dir, et test_dir\n",
        "# train_dir contient train_cats_dir(1000 chats) et train_dogs_dir(1000 chiens)\n",
        "# val_dir contient val_cats_dir(500 chats) et val_dogs_dir(500 dogs)\n",
        "# test_dir contient test_cats_dir(500 dogs) et val_cats_dir(500 dogs)\n",
        "import os, shutil\n",
        "\n",
        "os.mkdir(\"base_dir\") # qui va contenir train, test et val\n",
        "\n",
        "train_dir = os.path.join(\"base_dir\", \"train_dir\")\n",
        "test_dir = os.path.join(\"base_dir\", \"test_dir\")\n",
        "val_dir = os.path.join(\"base_dir\", \"val_dir\")\n",
        "os.mkdir(train_dir)\n",
        "os.mkdir(test_dir)\n",
        "os.mkdir(val_dir)\n",
        "\n",
        "# train_dir\n",
        "train_cats_dir = os.path.join(train_dir, \"train_cats_dir\")\n",
        "train_dogs_dir = os.path.join(train_dir, \"train_dogs_dir\")\n",
        "os.mkdir(train_cats_dir)\n",
        "os.mkdir(train_dogs_dir)\n",
        "\n",
        "# val_dir\n",
        "val_cats_dir = os.path.join(val_dir, \"val_cats_dir\")\n",
        "val_dogs_dir = os.path.join(val_dir, \"val_dogs_dir\")\n",
        "os.mkdir(val_cats_dir)\n",
        "os.mkdir(val_dogs_dir)\n",
        "\n",
        "# Test dir\n",
        "test_cats_dir = os.path.join(test_dir, \"test_cats_dir\")\n",
        "test_dogs_dir = os.path.join(test_dir, \"test_dogs_dir\")\n",
        "os.mkdir(test_cats_dir)\n",
        "os.mkdir(test_dogs_dir)\n",
        "\n",
        "# copier 1000 chats dans train_cats_dir et 1000 chiens dans train_dogs_dir\n",
        "cnames = [f'cat.{i}.jpg' for i in range(1000)] # cats\n",
        "dnames = [f'dog.{i}.jpg' for i in range(1000)] # dogs\n",
        "for cat, dog in zip(cnames, dnames):\n",
        "  src_cat = os.path.join(original_dir, cat)\n",
        "  src_dog = os.path.join(original_dir, dog)\n",
        "  dest_cat = os.path.join(train_cats_dir, cat)\n",
        "  dest_dog = os.path.join(train_dogs_dir, dog)\n",
        "\n",
        "  shutil.copyfile(src_cat, dest_cat)\n",
        "  shtil.copyfile(src_dog, dest_dog)\n",
        "\n",
        "# copier 500 chats val_cats_dir et 500 chiens dans val_dogs_dir\n",
        "cnames = [f'cat.{i}.jpg' for i in range(1000, 1500)] # cats\n",
        "dnames = [f'dog.{i}.jpg' for i in range(1000, 1500)] # dogs\n",
        "for cat, dog in zip(cnames, dnames):\n",
        "  src_cat = os.path.join(original_dir, cat)\n",
        "  src_dog = os.path.join(original_dir, dog)\n",
        "  dest_cat = os.path.join(val_cats_dir, cat)\n",
        "  dest_dog = os.path.join(val_dogs_dir, dog)\n",
        "\n",
        "  shutil.copyfile(src_cat, dest_cat)\n",
        "  shtil.copyfile(src_dog, dest_dog)\n",
        "\n",
        "# copier 500 chats test_cats_dir et 500 chiens dans test_dogs_dir\n",
        "cnames = [f'cat.{i}.jpg' for i in range(1000)] # cats\n",
        "dnames = [f'dog.{i}.jpg' for i in range(1000)] # dogs\n",
        "for cat, dog in zip(cnames, dnames):\n",
        "  src_cat = os.path.join(original_dir, cat)\n",
        "  src_dog = os.path.join(original_dir, dog)\n",
        "  dest_cat = os.path.join(test_cats_dir, cat)\n",
        "  dest_dog = os.path.join(test_dogs_dir, dog)\n",
        "\n",
        "  shutil.copyfile(src_cat, dest_cat)\n",
        "  shtil.copyfile(src_dog, dest_dog)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTRbkrWv2VHG",
        "outputId": "8a87efb2-3d3b-41a1-835a-abf3434f522f"
      },
      "source": [
        "# Construction réseau de neurones\n",
        "\n",
        "input = tf.keras.layers.Input((150,150, 3))\n",
        "layer_1 = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(input)\n",
        "layer_2 = tf.keras.layers.MaxPool2D((2,2))(layer_1)\n",
        "layer_3 = tf.keras.layers.Conv2D(64, (3,3), activation='relu')(layer_2)\n",
        "layer_4 = tf.keras.layers.MaxPool2D((2,2))(layer_3)\n",
        "layer_5 = tf.keras.layers.Conv2D(128, (3,3), activation='relu')(layer_4)\n",
        "layer_6 = tf.keras.layers.MaxPool2D((2,2))(layer_5)\n",
        "layer_7 = tf.keras.layers.Flatten()(layer_6)\n",
        "layer_8 = tf.keras.layers.Dense(512, activation='relu')(layer_7)\n",
        "output = tf.keras.layers.Dense(1, activation= 'sigmoid')(layer_8)\n",
        "model = tf.keras.models.Model(input, output)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 148, 148, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 74, 74, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 72, 72, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 36, 36, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 34, 34, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 17, 17, 128)       0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 36992)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               18940416  \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 19,034,177\n",
            "Trainable params: 19,034,177\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izGZ4lDW5EJu"
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBf1V-l0_mOa"
      },
      "source": [
        "Le modèle si on l'entraîne avec ce jeu de données va sur-apprendre."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG_eeUXx543Z"
      },
      "source": [
        "# Preprocessing data\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Normaliser les image\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=train_set,\n",
        "    target_size=(255,55), # redimensionner toutes les images\n",
        "    class_mode='binary', # deux classes\n",
        "    batch_size = 20\n",
        ")\n",
        "\n",
        "val_generator = test.datagen.flow_from_directory(\n",
        "    directory=val_set,\n",
        "    target_size=(255,55), # redimensionner toutes les images\n",
        "    class_mode='binary', # deux classes\n",
        "    batch_size = 20\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk6YFJw_81RQ"
      },
      "source": [
        "for data, label in train_generator:\n",
        "  print(\"Batch size: \", data.shape)\n",
        "  print(\"Label size: \", label.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14rihErc-qs-"
      },
      "source": [
        "history = model.fit_generator(train_generator, \n",
        "                              steps_per_epoch=100,\n",
        "                              epochs=30,\n",
        "                              validation_data=val_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZPRy0ZN_Jw0"
      },
      "source": [
        "train_acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNHcBqrC_uCt"
      },
      "source": [
        "On essaie de faire un **Data Augmentation** pour générer plus d'images à partir des 1000 images du train en faisant un des rotations, des zooms, des translations, ...\n",
        "\n",
        "On applique un **Data Augmentation** que sur le train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWUUnoY8_9yc"
      },
      "source": [
        "# Application du Data augmentation - exemple\n",
        "datagen = ImageDataGenerator(rotation_range=40, # on applique 40 rotations, donc on génére 40 images\n",
        "                             width_shift_range=0.2, # translation horizontale\n",
        "                             height_shift_range=0.2, # translation verticale\n",
        "                             shear_range=0.2, # translation inclinée\n",
        "                             horizontal_flip=True,\n",
        "                             fill_mode='nearest'\n",
        "                             )\n",
        "\n",
        "fnames = [cat for cat in os.listdir(train_cats_dir)]\n",
        "path = fnames[0]\n",
        "from keras.preprocessing import image\n",
        "cat_1 = image.load_img(path=path, target_size=(150, 150))\n",
        "x = image.img_to_array(cat_1)\n",
        "x=x.reshape((1,)+x.shape)\n",
        "i=0\n",
        "# Afficher l'image sous les 40 rotations\n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "  plt.figure(i)\n",
        "  plt.imshow(image.array_to_img(batch[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_lKeW1MJnFJ"
      },
      "source": [
        "# On refait notre preprocessing et pour chaque \n",
        "# image du train, on applique 40 data augmentation\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "                             rescale=1./255,\n",
        "                             rotation_range=40, \n",
        "                             width_shift_range=0.2,\n",
        "                             height_shift_range=0.2, \n",
        "                             zoom_range=0.2,\n",
        "                             horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=train_set,\n",
        "    target_size=(255,55), # redimensionner toutes les images\n",
        "    class_mode='binary', # deux classes\n",
        "    batch_size = 20\n",
        ")\n",
        "\n",
        "val_generator = test.datagen.flow_from_directory(\n",
        "    directory=val_set,\n",
        "    target_size=(255,55), # redimensionner toutes les images\n",
        "    class_mode='binary', # deux classes\n",
        "    batch_size = 20\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9ervsBmKZ-q"
      },
      "source": [
        "# Construction réseau de neurones\n",
        "\n",
        "input = tf.keras.layers.Input((150,150, 3))\n",
        "layer_1 = tf.keras.layers.Conv2D(32, (3,3), activation='relu')(input)\n",
        "layer_2 = tf.keras.layers.MaxPool2D((2,2))(layer_1)\n",
        "layer_3 = tf.keras.layers.Conv2D(64, (3,3), activation='relu')(layer_2)\n",
        "layer_4 = tf.keras.layers.MaxPool2D((2,2))(layer_3)\n",
        "layer_5 = tf.keras.layers.Conv2D(128, (3,3), activation='relu')(layer_4)\n",
        "layer_6 = tf.keras.layers.MaxPool2D((2,2))(layer_5)\n",
        "layer_7 = tf.keras.layers.Flatten()(layer_6)\n",
        "layer_8 = tf.keras.layers.Dense(512, activation='relu')(layer_7)\n",
        "output = tf.keras.layers.Dense(1, activation= 'sigmoid')(layer_8)\n",
        "model = tf.keras.models.Model(input, output)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwmTQ55cKeot"
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szGhUc8VKvGZ"
      },
      "source": [
        "history = model.fit_generator(train_generator, \n",
        "                              steps_per_epoch=100,\n",
        "                              epochs=30,\n",
        "                              validation_data=val_generator)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}