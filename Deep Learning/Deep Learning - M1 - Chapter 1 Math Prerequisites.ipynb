{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep Learning - M1 - Chapter 1 Math Prerequisites.ipynb","provenance":[{"file_id":"1fdqX_yAbjt7qJ7Cry__g3QLs8zIcZwWg","timestamp":1615116894939}],"collapsed_sections":["CCVPW79BNuiA","4jiMC6VlNunW","ZIvA9pYTNurf","kIByhKiT2PmP","kMRs1SDiRTZZ","kOZUaTwD2PmV","A1w4-6fw2Pml","_lUCxpC3eQf4","FoaJKfWV2Pmx","0sbIBEM2Wnpl","lepVjg5I2Pm1","InLUKBZG2Pm4","X7QKXhk3mUjt","0_PZt5fSI0Tp","ilRKtX6UxPPS","MhCHPhuxI0Tz","3VhKM8rLI0T5","UQlg_04hI0T8"],"authorship_tag":"ABX9TyN5FfPD1QgHweJ8/2jQ14N4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CCVPW79BNuiA"},"source":["# Introduction into Mathematical Modelling"]},{"cell_type":"markdown","metadata":{"id":"KW597MT12Pl_"},"source":["* Consider predicting person's **height** *from* its **age**:\n","\n","\n","* Mathematically, this task translates to:\n","\n","\\\\\n","\n","   \\begin{equation}\n","      \\mbox{height} = f(\\mbox{age}),\n","   \\end{equation}\n","\n","\\\\\n","\n"," where $f$ is logarithmic function with parameters\n","\n","\\\\\n","\n","   \\begin{equation}\n","    f(\\mbox{age}) = 2.2 \\log \\left(80 \\times \\mbox{age} \\right). \n","   \\end{equation}\n","\n","\\\\\n","\n"," ![](https://drive.google.com/uc?export=view&id=1vBEmQ5NtQxUv1wcma2NiU7kvw2US2Y9n)"]},{"cell_type":"markdown","metadata":{"id":"OePH7yhx2PmA"},"source":["* In model \n","\n","\\\\\n","\n","\\begin{equation}\n","    f(\\mbox{age}) = 2.2 \\log \\left(80 \\times \\mbox{age} \\right)\n","\\end{equation}\n","\n","\\\\\n","\n","* both input and output (age and height) are **single numbers**;\n","\n","\n","* In case where input and output are **vectors**, the model would have been expressed with ***matrices***;\n","\n","\n","* **Linear Algebra** is the branch of mathematics that study linear transformations between vectors in **linear** fashion;\n","\n","\n","* In Deep Leqrning, models are built over $4$ algebraic objects: \n","    1. **scalars**;\n","    2. **vectors**;\n","    3. **matrices**;\n","    4. **tensors**."]},{"cell_type":"markdown","metadata":{"id":"4jiMC6VlNunW"},"source":["# 1. Linear Algebra Prerequisites"]},{"cell_type":"markdown","metadata":{"id":"ZIvA9pYTNurf"},"source":["## Scalars"]},{"cell_type":"markdown","metadata":{"id":"pGwRY-NC2PmC"},"source":["* Scalars are single numbers  defined by the set to which they belong;\n","\n","* For example, number of persons in the classroom is $p \\in {\\rm I\\!N}$ while the temperature is $t \\in {\\rm I\\!R}$;\n","\n","* In Python, scalar are defined as"]},{"cell_type":"code","metadata":{"id":"0CdjyaDa2PmD","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603266812067,"user_tz":-120,"elapsed":727,"user":{"displayName":"Nedjmeddine Allab","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsF4-sjDEXHeg4BDbfx-C0agOdsvP8pVB9JXhGig=s64","userId":"01669104623581704181"}},"outputId":"e6d7f09a-1e67-442c-b88e-7dca03cae5e3"},"source":["n = 52\n","t = 12.5\n","print(\"we have \" + str(n) + \" students and the temperature is \" + str(t) + \" degrees.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["we have 52 students and the temperature is 12.5 degrees.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kIByhKiT2PmP"},"source":["## Vectors"]},{"cell_type":"markdown","metadata":{"id":"kMRs1SDiRTZZ"},"source":["### Definition"]},{"cell_type":"markdown","metadata":{"id":"zgQHsroo2PmQ"},"source":["* Vectors are ordered array of numbers and can represent all sort of data (image, video, text,...).\n","\n","* Let $\\bf{x}$ be a vector with $d$ elements\n","\n","\\begin{equation}\n","\\bf{x} = \n","\\begin{bmatrix}\n","    x_1\\\\\n","    \\vdots\\\\\n","    x_d\\\\\n","\\end{bmatrix}.\n","\\\\\n","\\end{equation}\n","\n","\n","* If $x_1,\\ldots, x_d \\in {\\rm I\\!R}$ then we write $\\bf{x} \\in {\\rm I\\!R}^d$;\n","\n","* Addition and multiplication by scalers must always produce vectors;\n"]},{"cell_type":"markdown","metadata":{"id":"W-6Br8oC2PmS"},"source":["\n","* Let $\\bf{x}$ and $\\bf{y}$ be two vectors defined in ${\\rm I\\!R}^2$ such that  \n","\n","\\begin{equation}\n","\\bf{x} = \n","\\begin{bmatrix}\n","    3\\\\\n","    1\\\\\n","\\end{bmatrix},\n","\\bf{y} = \n","\\begin{bmatrix}\n","    2\\\\\n","    3\\\\\n","\\end{bmatrix}.\n","\\end{equation}\n","\n","* $\\bf{x}$ and $\\bf{y}$ are geometrically represented as follows\n","\n"," ![](https://drive.google.com/uc?export=view&id=13ajZipJkZ8oTIDsT-4skOO661_5qfsds)\n","\n","* Vectors are essentially characterized by:\n","\n","  1. **Direction**; \n","\n","  2. **Length** obtained with Pythagorean theorem:\n","\n","  \\begin{equation}\n","  \\vert\\vert \\bf{x}\\vert\\vert_2 = \\sqrt(\\sum_{i = 1}^{2} x_i^2).\n","  \\end{equation}\n"]},{"cell_type":"markdown","metadata":{"id":"GUtvRRQGXv1M"},"source":["* Vectors **are not points**:\n","  * Vectors are objects characterized by direction and length;\n","  * Points have no direction and their length is equal to zero;\n"," \n","* For readability reasons we tend to **represent** vectors as **points** but keep in mind these points are related to the origin and possess direction and length."]},{"cell_type":"markdown","metadata":{"id":"laDB2G_vXv1Q"},"source":[" ![](https://drive.google.com/uc?export=view&id=1Dn1fcTv1RRvHFH9TxDVsKlyoqEsSZqXL)\n"," "]},{"cell_type":"markdown","metadata":{"id":"kOZUaTwD2PmV"},"source":["### Operations"]},{"cell_type":"markdown","metadata":{"id":"Btm_gSgJ2PmW"},"source":["#### **Transposition**\n","\n","* Let $\\bf{x} \\in {\\rm I\\!R}^d$, **By default**, $\\bf{x}$ is defined as **column** vector;\n","\n","\\begin{equation}\n","\\bf{x} = \n","\\begin{bmatrix}\n","    x_1\\\\\n","    \\vdots\\\\\n","    x_d\\\\\n","\\end{bmatrix}.\n","\\end{equation}\n","\n","\n","* Vectors can also be defined as ***row*** like \n","\n","\\begin{equation}\n","\\bf{y} = \\left[ y_1,\\ldots, y_d \\right].\n","\\\\\n","\\end{equation}\n","\n","* Column and row versions of a same vector are not the same and relate through **transpose** operator;\n","\n","* $\\bf{x}$ transpose is written $\\bf{x}^T$. **Column** vector can be written $\\bf{x} = \\left[ x_1,\\ldots, x_d \\right]^T$. "]},{"cell_type":"markdown","metadata":{"id":"DfMwYiDa2PmY"},"source":["#### **Scalar Multiplication**\n","\n","* Let $\\alpha \\in {\\rm I\\!R}$ be a scalar and $\\bf{x} \\in {\\rm I\\!R}^m$, $\\bf{x}$ can be ***scaled*** by $\\alpha$ as follows\n","\n","\\begin{equation}\n","\\alpha \\bf{x} = \n","\\begin{bmatrix}\n","\\alpha x_1\\\\\n","\\vdots\\\\\n","\\alpha x_m\n","\\end{bmatrix}.\n","\\end{equation}\n","\n","* Let $\\alpha = 2, \\beta = -\\frac{1}{2}$ be two scalars respectively applied on  $\\bf{x}$, $\\bf{y} \\in {\\rm I\\!R}^2$   \n","\n","\\begin{equation}\n","\\bf{x} = \n","\\begin{bmatrix}\n","    3\\\\\n","    1\\\\\n","\\end{bmatrix}, \\quad\n","\\bf{y} = \n","\\begin{bmatrix}\n","    2\\\\\n","    3\\\\\n","\\end{bmatrix}.\n","\\end{equation}\n","\n","* Scaled vectors are \n","\n","\\begin{equation}\n","\\alpha\\bf{x} = \n","\\begin{bmatrix}\n","    6\\\\\n","    2\\\\\n","\\end{bmatrix}, \\quad\n","\\beta \\bf{y} = \n","\\begin{bmatrix}\n","    -1\\\\\n","    -\\frac{3}{2}\\\\\n","\\end{bmatrix}.\n","\\end{equation}.\n","\n"," ![](https://drive.google.com/uc?export=view&id=1SAEJfi3j5mtswhm7iRJ80UdvhuObR4wQ)\n"]},{"cell_type":"markdown","metadata":{"id":"J0y61FqH2PmZ"},"source":["#### **Unit Vectors**\n","\n","* Let $\\bf{x} \\in {\\rm I\\!R}^m$. The unit-vector of $\\bf{x}$ is $\\bf{u}$ defined as \n","\n","\\begin{equation}\n","\\bf{u} = \\frac{1}{\\vert\\vert \\bf{x} \\vert\\vert} \\bf{x}.\n","\\end{equation}\n","\n","* $\\bf{u}$ has same direction as $\\bf{x}$ with $\\vert\\vert \\bf{u} \\vert\\vert = 1$. "]},{"cell_type":"markdown","metadata":{"id":"keNaOK7w2Pmc"},"source":["#### **Addition**\n","\n","* Let  $\\bf{x}$ and $\\bf{y} \\in {\\rm I\\!R}^m$.\n","\n","* Addition and subtraction between $\\bf{x}$ and $\\bf{y}$ are respectively defined as\n","\n","\\begin{equation}\n"," \\bf{x} + \\bf{y} = \n","\\begin{bmatrix}\n"," x_1 + y_1\\\\\n","\\vdots\\\\\n"," x_m + y_m\n","\\end{bmatrix},\n"," \\bf{x} - \\bf{y} = \n","\\begin{bmatrix}\n"," x_1 - y_1\\\\\n","\\vdots\\\\\n"," x_m - y_m\n","\\end{bmatrix}, \n","\\end{equation}\n","\n"," ![](https://drive.google.com/uc?export=view&id=1L4mp3yfTIGh330zVotYH4Klbt5VnZPeG)"]},{"cell_type":"markdown","metadata":{"id":"8GnxRlkh2Pmd"},"source":["#### **Dot Product**\n","\n","* Dot product between $\\bf{x}$ and $\\bf{y} \\in {\\rm I\\!R}^m$ is  ***scalar*** defined as \n","\n","\\begin{equation}\n","\\bf{x}.\\bf{y} = \\vert\\vert\\bf{x}\\vert\\vert \\times \\vert\\vert \\bf{y}\\vert\\vert \\times \\cos(\\theta),\n","\\end{equation}\n","\n","where $\\theta$ is the angle between $\\bf{x}$ and $\\bf{y}$; \n","\n","* $\\cos(\\theta) \\in [-1, 1]$ measures how much $\\bf{x}$ and $\\bf{y}$ are oriented in terms of direction;\n","\n","* A more convenient way to compute dot-product is the algebraic formula\n","\n","\\begin{equation}\n","\\bf{x}.\\bf{y} = \\sum_{i = 1}^{m} x_i y_i.\n","\\end{equation}\n"]},{"cell_type":"markdown","metadata":{"id":"3SSP7XrL2Pme"},"source":["#### **Illustration of Dot Product**\n","\n","\n","\\begin{equation}\n","\\bf{x}.\\bf{y} = \\vert\\vert\\bf{x}\\vert\\vert \\times \\vert\\vert \\bf{y}\\vert\\vert \\times \\cos(\\theta).\n","\\\\\n","\\end{equation}\n","\n","* $\\cos(\\theta)$ measures similarity between $\\bf{x}$ and $\\bf{y}$ in terms of direction where:\n","  * $\\cos(\\theta) = 1$ means perfect alignement\n","  * whereas $\\cos(\\theta) = -1$ means perfect opposition. \n","\n","\n"," \n","* Consider the following dot products:\n"," \n","  * $\\bf{x},\\bf{y}$ are perfectly aligned resulting into $\\cos(\\theta) = 1$;\n","\n","  * $\\bf{x},\\bf{p}$ and $\\bf{x},\\bf{u}$ are perpendicular resulting into null dot products because of $\\cos(\\frac{\\pi}{2}) = \\cos(\\frac{3\\pi}{2}) = 0$;\n","\n","  * $\\bf{x},\\bf{z}$ are aligned but look into opposite direction producing $\\cos(\\theta) = -1$;\n","\n","  * vectors forming angle  $\\theta \\in \\left] \\frac{3\\pi}{2},\\frac{\\pi}{2} \\right[$ with $\\bf{x}$ **tend** to the direction of $\\bf{x}$;\n","\n","  * inversely, vectors forming angle  $\\theta \\in \\left] \\frac{\\pi}{2},\\frac{3\\pi}{2} \\right[$ with $\\bf{x}$ evolve in opposite direction to $\\bf{x}$.\n","\n"," ![](https://drive.google.com/uc?export=view&id=13LCU8vo1zUba4GpMyOSVsIu-cumnX-2w)"]},{"cell_type":"markdown","metadata":{"id":"PndbtlRJ2Pmf"},"source":["### Vectors in Python\n","\n"]},{"cell_type":"code","metadata":{"id":"2k943Op92Pmg","outputId":"75558bf5-7dee-4aa5-d6c2-8451651518d0"},"source":["import numpy as np\n","x = np.array([3,1], dtype = np.float)\n","y = np.array([2,3], dtype = np.float)\n","\n","ax = 2 * x \n","print('scaled x by 2 = ')\n","print(ax)\n","\n","a = x + y\n","print('\\nx + y = ')\n","print(a)\n","\n","s = x-y\n","print('\\nx - y = ')\n","print(s)\n","\n","x = np.array([3,1], dtype = np.float)\n","y = np.array([2,3], dtype = np.float)\n","\n","d = x.dot(y)\n","print('\\nthe dot product x.y = ' + str(d))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["scaled x by 2 = \n","[6. 2.]\n","\n","x + y = \n","[5. 4.]\n","\n","x - y = \n","[ 1. -2.]\n","\n","the dot product x.y = 9.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A1w4-6fw2Pml"},"source":["## Matrices"]},{"cell_type":"markdown","metadata":{"id":"_lUCxpC3eQf4"},"source":["### Definition"]},{"cell_type":"markdown","metadata":{"id":"ezgw0iee2Pmm"},"source":["\n","\n","* Visually, matrices tend to be defined as grid of numbers where elements are identified by row and column indices;\n","\n","* Algebraically, matrices are functions that **linearly transform** one vector into another;\n","\n","* Let $\\bf{W}$ be matrix of $m$ rows and $d$ columns\n","\n","\n","\\begin{equation}\n","\\bf{W} =\n","\\begin{bmatrix}\n","w_{11} & \\ldots & w_{1d} \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","w_{m1} & \\ddots & w_{md}\n","\\end{bmatrix}.\n","\\\\\n","\\end{equation}\n","\n","\n","*  $\\bf{W}$ transforms input vectors $\\bf{x} \\in \\rm I\\!R^d$ to output vectors $\\bf{y} \\in \\rm I\\!R^m$;\n","\n"]},{"cell_type":"markdown","metadata":{"id":"u1I7Ce3N2Pmn"},"source":["* Let $\\bf{A}$ be a $2 \\times 2$ matrix and  an input vector $\\bf{x}$ where \n","\n","\\begin{equation}\n","\\bf{A} =\n","\\begin{bmatrix}\n","-2 & 0\\\\\n","0 & -2\n","\\end{bmatrix},\n","\\quad \\bf{x} = \n","\\begin{bmatrix}\n","    3\\\\\n","    1\\\\\n","\\end{bmatrix}\n","\\end{equation}\n","\n","* We define $\\bf{y}$ as the linear transformation of $\\bf{x}$ through $\\bf{A}$\n","\n","\\begin{equation}\n","\\bf{y} = \n","3 \n","\\begin{bmatrix}\n","    -2\\\\\n","    0\\\\\n","\\end{bmatrix}+1\n","\\begin{bmatrix}\n","    0\\\\\n","    -2\\\\\n","\\end{bmatrix}=\n","\\begin{bmatrix}\n","-6\\\\\n","-2\n","\\end{bmatrix}.\n","\\end{equation}\n"]},{"cell_type":"markdown","metadata":{"id":"zj7tKDip2Pmo"},"source":["### Operations"]},{"cell_type":"markdown","metadata":{"id":"yKrrzTvD2Pmp"},"source":["#### Transposition\n","\n","* Let $\\bf{A} \\in \\rm I\\!R^m \\times \\rm I\\!R^d$ or ($\\rm I\\!R^{m \\times d}$) be matrix with $m$ rows and $d$ columns;\n","\n","\\begin{equation}\n","\\bf{A} =\n","\\begin{bmatrix}\n","a_{11} & \\ldots & a_{1d} \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","a_{m1} & \\ddots & a_{md}\n","\\end{bmatrix}.\n","\\end{equation}\n","\n","* Transposed $\\bf{A}$ noted $\\bf{A}^T$ is a matrix with $d$ rows and $m$ columns where each $j$-th column vector in $\\bf{A}^T$ corresponds to the $j$-th row vector in $\\bf{A}$;   \n","\n","\\begin{equation}\n","\\bf{A}^T =\n","\\begin{bmatrix}\n","a_{11} & \\ldots & a_{m1} \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","a_{1d} & \\ddots & a_{md}\n","\\end{bmatrix}.\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"b1Ztr3n52Pmq"},"source":["#### Scalar Matrix Multiplication\n","\n","* Let $\\alpha \\in \\rm I\\!R$ and $\\bf{A} \\in \\rm I\\!R^m \\times \\rm I\\!R^d$,  product $\\alpha \\bf{A}$ is defined as\n","\n","\\begin{equation}\n","\\alpha \\bf{A} =\n","\\begin{bmatrix}\n","\\alpha a_{11} & \\ldots & \\alpha a_{1d} \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","\\alpha a_{m1} & \\ddots & \\alpha a_{md}\n","\\end{bmatrix}.\n","\\end{equation}\n","\n","* $\\alpha \\bf{A} = \\bf{A} \\alpha $ "]},{"cell_type":"markdown","metadata":{"id":"zXFlxIbR2Pmr"},"source":["#### Vector Matrix Multiplication\n","\n","* Let $\\bf{x} \\in \\rm I\\!R^d$ and $\\bf{A} \\in \\rm I\\!R^m \\times \\rm I\\!R^d$,  as described above, $\\bf{A} \\bf{x} \\$ is merely the sum\n","\n","\\begin{equation}\n","\\bf{A} \\bf{x} =\n","x_1\n","\\begin{bmatrix}\n","a_{11}\\\\\n","\\vdots\\\\\n","a_{m1}\n","\\end{bmatrix}\n","+\\cdots+\n","x_d\n","\\begin{bmatrix}\n","a_{m1}\\\\\n","\\vdots\\\\\n","a_{md}\n","\\end{bmatrix}.\n","\\end{equation}\n","\n","\n","* The weighted sum above is compactly expressed as sums of products between elements of the vector with elements of matrix rows \n","\n","\n","\n","\\begin{equation}\n","\\bf{A}\\bf{x} =\n","\\begin{bmatrix}\n","\\sum_{j = 1}^m a_{1j} x_j\\\\\n","\\vdots\\\\\n","\\sum_{j = 1}^m a_{mj} x_j\\\\\n","\\end{bmatrix}.\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"I_B-g8jW2Pmr"},"source":["#### Illustration of Vector Matrix Multiplication\n","\n","* Let matrices $\\bf{A}, \\bf{B}$ and vector $\\bf{x}$ \n","\n","\\begin{equation}\n","\\bf{A} = \n","\\begin{bmatrix}\n","2 & 0 \\\\\n","0 & 2\n","\\end{bmatrix}, \\quad\n","\\bf{B} =\n","\\begin{bmatrix}\n","-2 & 0\\\\\n","0 & -2\n","\\end{bmatrix}, \\quad\n","\\bf{x} = \n","\\begin{bmatrix}\n","3\\\\\n","1\n","\\end{bmatrix}.\n","\\end{equation}\n","\n","![](https://drive.google.com/uc?export=view&id=1XKBEF6v14HFyK6okt5bRYX7_G3DV6c9t)\n"]},{"cell_type":"markdown","metadata":{"id":"GU9K0teX2Pms"},"source":["#### Matrix Matrix Multiplication\n","\n","* Product between matrices follows the same principle as matrice vector product;\n","\n","* Consider $2 \\times 2$ matrices $\\bf{A}$ and $\\bf{B}$;\n","\n","\\begin{equation}\n","\\bf{A} =\n","\\begin{bmatrix}\n","a_{11} & a_{12} \\\\\n","a_{21} & a_{22}\n","\\end{bmatrix},\n","\\quad\n","\\bf{B} =\n","\\begin{bmatrix}\n","b_{11} & b_{12} \\\\\n","b_{21} & b_{22}\n","\\end{bmatrix}. \n","\\end{equation}\n","\n","* The product $\\bf{A}\\bf{B}$ is\n","\n","\\begin{equation}\n","\\bf{A}\\bf{B} = \n","\\begin{bmatrix}\n","a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22} \\\\\n","a_{21}b_{11} + a_{22}b_{21} & a_{21}b_{12} + a_{22}b_{22}\n","\\end{bmatrix},\n","\\end{equation}\n","\n","* Matrix multiplication can be generalized:\n","  * Let $\\bf{A} \\in \\rm I\\!R^{m \\times d}$ and $\\bf{B} \\in \\rm I\\!R^{d \\times n}$;\n","\n","  * Note that the number of columns in $\\bf{A}$ **must be equal** to the number of rows in $\\bf{B}$;\n","\n","  * The dimension of the resulting matrix is $m \\times n$ \n","\n","\n","\\begin{equation}\n","\\bf{A} \\times \\bf{B} = \n","\\begin{bmatrix}\n","\\sum_{j = 1} ^d a_{1j}b_{1j} & \\ldots & \\sum_{j = 1} ^d a_{1j}b_{nj} \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","\\sum_{j = 1} ^d a_{mj}b_{1j} & \\ldots & \\sum_{j = 1} ^d a_{mj}b_{nj}\n","\\end{bmatrix}.\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"bPA5G2x82Pmv"},"source":["#### Few Properties on operations between Matrices \n","\n","\n","\n","\\begin{equation}\n","\\bf{A}( \\bf{B} + \\bf{C} ) = \\bf{A}\\bf{B} + \\bf{A}\\bf{C}.\n","\\end{equation}\n","\n","\n","\n","* Multiplication is associative:\n","\n","\n","\n","\\begin{equation}\n","\\bf{A} \\left(\\bf{B}\\bf{C} \\right) = \\left(\\bf{A}\\bf{B}\\right) \\bf{C}. \n","\\end{equation}\n","\n","\n","\n","* Multiplication is not commutative:\n","\n","\n","\n","\\begin{equation}\n","\\bf{A}\\bf{B}  \\neq \\bf{B}\\bf{A}. \n","\\end{equation}\n","\n","\n","* Transpose of multiplication:\n","\n","\n","\n","\\begin{equation}\n","\\left(\\bf{A}\\bf{B}\\right)^T  = \\bf{B}^T\\bf{A}^T. \n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"8F8MlDlaXKgJ"},"source":["### Special Matrices"]},{"cell_type":"markdown","metadata":{"id":"y6a0RFWk-yw4"},"source":["* Matrices are defined by their dimensions but also by the values inside the cells;\n","\n","* There are some types of matrices with useful properties;\n"," "]},{"cell_type":"markdown","metadata":{"id":"cXgrMiFislC8"},"source":["**Diagonal**"]},{"cell_type":"markdown","metadata":{"id":"VjH8bJj2snor"},"source":["* Diagonal matrix are square with null values except in the diagonal\n","\n","\\\\\n","\n","\\begin{equation}\n","\\bf{D} =\n","\\begin{bmatrix}\n","d_{11} & \\ldots & 0 \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","0 & \\ddots & d_{dd}\n","\\end{bmatrix}.\n","\\end{equation}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZbyIB67JteLC"},"source":["**Identity**"]},{"cell_type":"markdown","metadata":{"id":"mInqvQZ6tjT1"},"source":["* Identity matrices are diagonal matrices with $1$s in the diagonal;\n","\n","* Identity matrice preserve input vectors both in direction and in length."]},{"cell_type":"markdown","metadata":{"id":"MdRXgHzoulCx"},"source":["**Inverse**"]},{"cell_type":"markdown","metadata":{"id":"sjACKOv_uqcl"},"source":["* Let $\\bf{A} \\in \\rm I\\!R^m \\times \\rm I\\!R^d$ be a matrix with $m$ rows and $d$ columns;\n","\n","\\\\\n","\n","* We define $\\bf{A}^{-1} \\in \\rm I\\!R^d \\times \\rm I\\!R^m$ as the inverse of $\\bf{A}$ that is:\n","\n","\\\\\n","\n","\\begin{equation}\n","\\bf{A}^{-1} \\bf{A} = \\bf{I}.\n","\\end{equation}\n","\n","\\\\\n","\n","* Existence of $\\bf{A}^{-1}$ must satisfy:\n","\n","  * $\\bf{A}$ must be square (number of rows = number of columns);\n","\n","  * no linear dependence amongst columns of $\\bf{A}$. "]},{"cell_type":"markdown","metadata":{"id":"JYFNsKaSyh-f"},"source":["**Symmetry**"]},{"cell_type":"markdown","metadata":{"id":"DGvTUYlhyoOe"},"source":["* Symmetric matrix is any matrix that is equal to its own transpose:\n","\n","\\\\\n","\n","\\begin{equation}\n","\\bf{A} = \\bf{A}^T.\n","\\end{equation}\n","\n","\\\\\n","\n","* Symmetric matrices often arise when entries are generated by some functions of two arguments that does not depend on the order of  arguments such as Covariance or Distance between data."]},{"cell_type":"markdown","metadata":{"id":"l2HGvEiT1wRI"},"source":["**Orthonormality**"]},{"cell_type":"markdown","metadata":{"id":"793uvwy91wYd"},"source":["* Orthogonal matrix is square whose:\n","\n","  * rows are mutually **orthonormal**; \n","\n","  * columns are mutually **orthonormal**.\n","\n","\\\\\n","\n","\\begin{equation}\n","\\bf{A}^T \\bf{A} = \\bf{A} \\bf{A}^T = \\bf{I}.\n","\\end{equation}\n","\n","* The equation above implies that:\n","\n","\\\\\n","\n","\\begin{equation}\n","\\bf{A}^{-1} = \\bf{A}^T.\n","\\end{equation}\n"]},{"cell_type":"markdown","metadata":{"id":"FoaJKfWV2Pmx"},"source":["### Matrix/Vector Spaces"]},{"cell_type":"markdown","metadata":{"id":"0sbIBEM2Wnpl"},"source":["#### **Determinant**"]},{"cell_type":"markdown","metadata":{"id":"uQqcu5dp_CJt"},"source":["* Matrix $\\bf{A}$ may transform single vectors $\\bf{x}$ but it can also transform whole **vector space** when applied on every vector inside that space;\n","\n","* For instance, let $\\bf{A}$ be the $2 \\times 2$ matrix \n","\n","\\\\\n","\n","\\begin{equation}\n","\\bf{A} = \n","\\begin{bmatrix}\n","3 & 1 \\\\\n","1 & 2\n","\\end{bmatrix}.\n","\\end{equation}\n","\n","\\\\\n","\n","* Below we visualize original basis vectors $\\bf{OC}$ and $\\bf{OA}$ with their respective transformations $\\bf{OF}$ and $\\bf{OD}$;\n","\n","\\\\\n","\n","* It is easy to see that all input vectors laying inside the square $\\bf{OABC}$ will end up inside the parallelogram $\\bf{ODEF}$;\n","  "]},{"cell_type":"markdown","metadata":{"id":"8e2xj22X_FTf"},"source":[" ![](https://drive.google.com/uc?export=view&id=1O_nMth8P4Kg6fRB94yyAX3oKJOrG8Kiq)\n"]},{"cell_type":"markdown","metadata":{"id":"mSowBpAPeNKz"},"source":["\n","* Matrix determinant $\\bf{det}(\\bf{A})$ is a scalar value\n","\n","  * Whose value measures the ratio between the areas of $\\bf{ODEF}$ and $\\bf{OABC}$;\n","\n","  * Whose sign is positive if the order between the basis vectors is unchanged and negative otherwise.\n","\n","  * Whose value is equal to $0$ if at least one column vector of $\\bf{A}$ is linearly dependent with the others (which leads to space contraction). \n","\n","\n","\\\\\n","\n","* For $2 \\times 2$ matrices\n","\n","\\\\\n","\n","\\begin{equation}\n","\\bf{A} = \n","\\begin{bmatrix}\n","a & b \\\\\n","c & d\n","\\end{bmatrix},\n","\\end{equation}\n","\n","\\\\\n","\n","* determinant is given by $\\bf{det}(A)$ $= ad - cb$.\n","\n","\\\\\n","\n","* For $3 \\times 3$ matrices \n","\n","\\begin{equation}\n","\\bf{A} = \n","\\begin{bmatrix}\n","a & b & c \\\\\n","d & e & f \\\\\n","g & h & i\n","\\end{bmatrix},\n","\\end{equation}\n","\n","\\\\\n","\n","\n","* the determinant is computed as follows:\n","\n","\\\\\n","\n","\\begin{equation}\n","\\bf{det(A)} = a .\\bf{det}\n","\\left(\n","\\begin{bmatrix}\n","e & f \\\\\n","h & i\n","\\end{bmatrix}\n","\\right)\n","- b .\\bf{det}\n","\\left(\n","\\begin{bmatrix}\n","d & f \\\\\n","g & i\n","\\end{bmatrix}\n","\\right)\n","+\n","c .\\bf{det}\n","\\left(\n","\\begin{bmatrix}\n","d & e \\\\\n","g & h\n","\\end{bmatrix}\n","\\right).\n","\\end{equation}\n","\n","\\\\\n","\n","* Same principle applies for higher dimensions. "]},{"cell_type":"markdown","metadata":{"id":"rugz8izeYYF9"},"source":["#### **Eigenvalues & Eigenvectors (Definitions)**"]},{"cell_type":"markdown","metadata":{"id":"nnM5t1Ff2Pmy"},"source":["\n","\n","\n","* When Matrix $\\bf{A}$ is applied on  input vectors it tends to alter its length and/or direction; \n","\n","\n","* Under some circumstances, certain vectors $\\bf{v}$ get altered **only in their length** that is\n","\n","\n","\\begin{equation}\n","\\bf{A} \\bf{v} = \\lambda \\bf{v}, \\quad \\lambda \\neq 0, \\bf{v} \\neq \\bf{0}.\n","\\end{equation}\n","\n","\n","* This kind of vectors are known as **eigenvectors** and the coefficient of the length alteration is the corresponding **eigenvalue**.\n","\n","\n","* Note that if $\\bf{v}$ is an eigenvector of $\\bf{A}$, then:\n","\n","  * So is any rescaled vector $s\\bf{v}$ for any $s \\neq 0$;\n","\n","  * $s\\bf{v}$ still has the same eigenvalue as $\\bf{v}$.\n","  \n","![](https://drive.google.com/uc?export=view&id=1wg79WpAbYO6Z0uL2uPpLjMuhGxHoNgdW)"]},{"cell_type":"markdown","metadata":{"id":"sPrLfpqT2Pm0"},"source":[" * Eigenvalues are computed through the **characteristic equation** defined as\n","\n","\\begin{equation}\n","\\begin{array}{11}\n","\\bf{A} \\bf{v}  & = & \\lambda \\bf{v} \\\\\n","\\bf{A} \\bf{v} - \\lambda \\bf{v} & = & 0,\\\\\n","\\end{array}\n","\\end{equation}\n","\n","* resulting into polynomial in $\\lambda$ where the roots are the eigenvalues of $\\bf{A}$.\n","\n","\\begin{equation}\n","\\bf{det} \\left(\\bf{A}  - \\lambda \\bf{I}\\right) = 0.\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"lepVjg5I2Pm1"},"source":["### Matrix Factorization"]},{"cell_type":"markdown","metadata":{"id":"BV-bvCT2ibPY"},"source":["#### **Eigendecomposition**"]},{"cell_type":"markdown","metadata":{"id":"qTW_3w852Pm2"},"source":["* Factorization splits linear transformations into atomic steps like distinguishing between rotations and length alteration.\n","\n","* Let $\\bf{A}$ a **diagonalizable matrix** (square, invertible and verify other conditions);\n","\n","\n","* If $\\bf{A}$ has $m$ linearly **independent** eigenvectors $\\{ \\bf{v}^{(1)},\\ldots,\\bf{v}^{(m)} \\}$ with corresponding eigenvalues $\\{ \\lambda_1, \\ldots, \\lambda_m\\}$ then  $\\bf{A}$ can be **factorized** as;\n","\n","\\\\\n","\n","\\begin{equation}\n","\\bf{A}  = \\bf{V} \\bf{\\Lambda} \\bf{V}^{-1}.\n","\\end{equation}\n","\n","\\\\\n","\n","* When $m$ eigenvectors are concatenated into matrix $\\bf{V}$ and the associated eigenvalues in the diagonal matrix $\\bf{\\Lambda}$, $\\bf{A}$ can be **factorized** as \n","\n","* Where $\\bf{V}$ is concatenation of  $m$ eigenvectors ordered by descending absolute values of eigenvalues.\n","\n","* If $\\bf{A}$ is **symmetric** the matrix of eigenvectors $\\bf{Q}$ is **orthogonal** and the factorization becomes\n","\n","\\\\\n","\n","\\begin{equation}\n","\\bf{A} = \\bf{Q}\\bf{\\Lambda}\\bf{Q}^T.\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"1_GHZobHkxOv"},"source":["#### **Singular Value Decomposition (SVD)**"]},{"cell_type":"markdown","metadata":{"id":"otjOmHlS2Pm2"},"source":["* Singular Value Decomposition (SVD) provides another way to factorize a matrix;\n","\n","* While eigendecomposition is not applicable to any matrix, SVD is;\n","\n","* SVD decomposition has lot of applications like Image compression, Recommender Systems (Netflix, Spotify, Amazon,...); \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wKprkxKR2Pm3"},"source":["\n","\n","* Let $\\bf{M}$ be a $m \\times d$ matrix. The SVD decomposition of $\\bf{M}$ is \n","\n","\\begin{equation}\n","\\bf{M} = \\bf{U} \\bf{\\Sigma} \\bf{V}^T,\n","\\end{equation}\n","\n","\n","* where for a given input vector $\\bf{x}$:\n","\n","  *  $\\bf{V}^T$ is $d \\times d$ **orthogonal** matrix and performs rotation on $\\bf{x}$ (we call $\\bf{x}_V$); \n","\n","  *  $\\bf{\\Sigma}$ is $m \\times d$ and performs streching of $\\bf{x_V}$ then projects it into $m$ dimensions (we call $\\bf{x}_{V\\Sigma}$);\n","\n","  *  $\\bf{U}$ is $m \\times m$ **orthogonal** matrix and performs last rotation on $\\bf{x}_{V\\Sigma}$. \n","\n","\n","![](https://drive.google.com/uc?export=view&id=1tG4H8p5ALEJpbar63U__8oLGbTUYGcqv)\n"]},{"cell_type":"markdown","metadata":{"id":"wiIr419G2Pm3"},"source":["* $\\bf{V}$ is constructed by the eigenvectors of $\\bf{M}^T\\bf{M}$;\n","\n","* $\\bf{U}$ is constructed by the eigenvectors of $\\bf{M}\\bf{M}^T$;\n","\n","* $\\bf{\\Sigma}$ is diagonal matrix whose entries are called ***singular values*** and correspond to square root of the eigenvalues of $\\bf{M}\\bf{M}^T$ or $\\bf{M}^T\\bf{M}$;\n","\n","* The number of **non-zero** singular values equals the rank of $\\bf{M}$;\n","\n","* If $\\bf{\\Sigma}$ is not square the rest of the diagonal are filled with zeros."]},{"cell_type":"markdown","metadata":{"id":"InLUKBZG2Pm4"},"source":["### Matrices in Python"]},{"cell_type":"code","metadata":{"id":"ZMnEUMSB2Pm5","outputId":"7819c553-4f61-4a6d-dbb3-2ba961c0c833"},"source":["import numpy as np\n","#Creation of a 2 X 2 matrix\n","A = np.array([[2,1],[1,2]], dtype = np.float32)\n","print('Matrix A has the dimension ' + str(A.shape))\n","print('with the entries')\n","print(A)\n","\n","\n","#Product between two matrices\n","B = np.array([[3,4,1], [4,7,0]])\n","AB = A.dot(B)\n","print('Dimension of A.B = ' + str(AB.shape))\n","print('\\nA.B = ')\n","print(AB)\n","\n","\n","#Transpose\n","ABt = AB.transpose()\n","print('Dimension of transposed AB = ' + str(ABt.shape))\n","print('\\nTransposed AB = ')\n","print(ABt)\n","\n","\n","#Diagonal Matrix\n","D = np.diag([1,2,3,4,5,6])\n","print(D)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Matrix A has the dimension (2, 2)\n","with the entries\n","[[2. 1.]\n"," [1. 2.]]\n","Dimension of A.B = (2, 3)\n","\n","A.B = \n","[[10. 15.  2.]\n"," [11. 18.  1.]]\n","Dimension of transposed AB = (3, 2)\n","\n","Transposed AB = \n","[[10. 11.]\n"," [15. 18.]\n"," [ 2.  1.]]\n","[[1 0 0 0 0 0]\n"," [0 2 0 0 0 0]\n"," [0 0 3 0 0 0]\n"," [0 0 0 4 0 0]\n"," [0 0 0 0 5 0]\n"," [0 0 0 0 0 6]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Z1as5OXE2Pm9","outputId":"ef457809-0753-4524-c49f-3bae8638793a"},"source":["import numpy as np\n","\n","#Determinant\n","A = np.array([[1, 2], [3, 4]])\n","detA = np.linalg.det(A)\n","print('Determinant of A = ' + str(detA))\n","\n","#eigenvalues & eigenvectors\n","A = np.array([[2,1],[1,2]], dtype = np.float32)\n","lambdas, V = np.linalg.eig(A)\n","print('eigenvalues = ')\n","print(lambdas)\n","print('eigenvectors = ')\n","print(V)\n","\n","#SVD decomposition\n","U, Sigma, Vt = np.linalg.svd(A)\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Determinant of A = -2.0000000000000004\n","eigenvalues = \n","[3. 1.]\n","eigenvectors = \n","[[ 0.70710677 -0.70710677]\n"," [ 0.70710677  0.70710677]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X7QKXhk3mUjt"},"source":["#2. Optimization/Calculus Prerequisites "]},{"cell_type":"markdown","metadata":{"id":"0_PZt5fSI0Tp"},"source":["## Introduction into Optimization"]},{"cell_type":"markdown","metadata":{"id":"-djcFDyfI0Tr"},"source":["* Mathematical optimization consists in maximizing or minimizing some function $f$ known as **objective function**.\n","\n","    * If for example $f$ measures business profit, optimization of $f$ seeks $x$ for which $f$ is the highest.\n","    \n","    * In case $f$ measures the error of mathematical model with respect to some parameter value $x$, optimization of $f$ looks for $x$ for which $f$ is the minimum.\n","\n","* Optimal values are known as **critical points** \n","\n","![](https://drive.google.com/uc?export=view&id=1KKBWOHSQk2E9qSpgG0X7TWi_KrSimE0f)"]},{"cell_type":"markdown","metadata":{"id":"OxJl-mQII0Ts"},"source":["* Objective function $f$ might have many critical points;\n","\n","* Critical points in such situations are said to be **local**;\n","\n","* The best optimum is called **global**.\n","\n","![](https://drive.google.com/uc?export=view&id=1vZ3xvIoQtINiTmlSrbnY1W3gj0AsiH4C)"]},{"cell_type":"markdown","metadata":{"id":"sXE6kozaI0Tu"},"source":["* Oftentimes $f$ depends on several variables. Below is an example of an objective function with two variables \n","\n","$$\n","f(x_1,x_2) = x_1^2 + x_2^2\n","$$\n","\n","* Minimizing  $f(x_1,x_2)$ means finding the values of $x_1,x_2$ that bring $f$  represented by the blue envelop hit the ground.\n","\n","![](https://drive.google.com/uc?export=view&id=1dC2BI7ZQ7I0ca9Ov8905nFcd_0deMfwD)"]},{"cell_type":"markdown","metadata":{"id":"ilRKtX6UxPPS"},"source":["## First Order Optimization"]},{"cell_type":"markdown","metadata":{"id":"tXRQxdVsx1a0"},"source":["* Unconstrained Optimization refers to the problems where no restriction is made over values that $x$ can take;\r\n","\r\n","\r\n","* Several methods exist to solve optimization problems.  We focus on Gradient Based approaches;"]},{"cell_type":"markdown","metadata":{"id":"MhCHPhuxI0Tz"},"source":["### Derivation (Intuition)\n","\n","* Consider the case where $x$ is scalar, critical points of $f$ can be found through mathematical tool called **derivation**.\n","\n","* Informally, derivative of $f$ at $x$ is the **inclination** known as **slope** of the line that touches $f(x)$. \n","\n","![](https://drive.google.com/uc?export=view&id=1QaraySjBXyfG11XllHl0KQ4FO_BTRwly)"]},{"cell_type":"markdown","metadata":{"id":"I8sdpx0OI0T0"},"source":["* The sign of the slope (positive, negative or nul) indicates from a given $x$ the direction where $f(x)$ is heading (up, down or no where):\n","\n","* Arrows below $f$ whose **inclinations** correspond to derivatives of $f$ at different $x$ (the length of the arrows has no particular meaning);\n","    \n","* Critical points (optimum) match with null derivatives;\n","\n","* Derivatives are defined as functions in $x$ which means identifying $x$ for which the derivative function equates zero take us to critical points;\n","\n","![](https://drive.google.com/uc?export=view&id=1arJWq2pSyLxkm-daUxZeEA2tncbIJIs3)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3RD-9xFcI0T2"},"source":["### Derivation  (Formal Definition)\n","\n","* Let $f(x)$ be real function differentiable at every $x$ (*** differentiable at every $x$ means smooth curve with no abrupt changes***). \n","\n","* We define $\\frac{d}{dx}$ as derivative operator applied to  $f$ with respect to $x$ such that\n","\n","\\begin{equation}\n","\\frac{df(x)}{dx} =  \\underset{h \\rightarrow 0}{\\mbox{lim}} \\frac{f(x + h) - f(x)}{(x + h) - x} = \\underset{h \\rightarrow 0}{\\mbox{lim}} \\frac{f(x + h) - f(x)}{h}.\n","\\end{equation}\n","\n","* The above equation produces  new function $f^{'}(x)$ that indicates for every $x$ the rate of change of $f(x)$ surrounding an **infinitesimally small** region near $x$.\n","\n","* When $h$ is very small, function $f$ is well-approximated with straight line and derivative is its slope.  "]},{"cell_type":"markdown","metadata":{"id":"c66pUOtQI0T3"},"source":["### Rules of Derivation\n","\n","* Let $f(x),g(x)$ be two differentiable functions and $a,b$ two reals. \n","\n","\\\\\n","\n","  1. **Constant Rule**:\n","  \\begin{equation}\n","  \\frac{d}{dx}(a) = 0.\n","  \\end{equation}\n","\n","\\\\\n","\n","  2. **Power Rule**:\n","  \\begin{equation}\n","   \\frac{d}{dx}(x^n) = n x^{n-1}.\n","  \\end{equation}\n","\n","\\\\\n","\n","  3. **Linearity Rules**:\n","\n","  \\begin{equation}\n","  \\left [a f(x)\\right]^{'} = a f^{'}(x). \\\\\n","  \\left[ f(x) \\pm  g(x) \\right]^{'} = f^{'}(x) \\pm g^{'}(x).\n","  \\end{equation}\n","    \n","\\\\\n","\n","  4. **Product Rule**:  \n","\n","\n","  \\begin{equation}\n","  \\left[f(x) \\times g(x)\\right]^{'} = f^{'}(x) \\times g(x) + f(x) \\times g^{'}(x). \n","  \\end{equation}\n","   \n","\\\\\n","\n","  5. **Quotient Rule**:  \n","\n","  \\begin{equation}\n","  \\left[\\frac{f(x)}{g(x)}\\right]^{'} = \\frac{f^{'}(x) \\times g(x) -  f(x) \\times g^{'}(x))}{g^{2}(x)}.\n","  \\end{equation}\n","    \n","\\\\\n","\n","  6. **Common Functions**:\n","  \\begin{equation}\n","  \\left[ \\log(x) \\right]^{'} = \\frac{1}{x},\\quad x > 0.\\\\\n","  \\left[ \\exp(x) \\right]^{'} = \\exp(x).\\\\\n","  \\left[ \\sin(x) \\right]^{'} =  \\cos(x).\\\\\n","  \\left[ \\cos(x)\\right]^{'} = - \\sin(x).\\\\\n","  \\left[ tanh(x)\\right]^{'} = 1 - tanh(x)^2\n","  \\end{equation}\n","  \n","  \n","\\\\\n","\n","  7. **Chain Rule**:\n","  \\begin{equation}\n","  \\frac{d}{dx} \\left( f(g(x))  \\right) = f^{'}\\left( g(x)\\right) g^{'}(x).\n","  \\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"C1YRRQxSI0T4"},"source":["### Gradient\n","\n","* Gradient is the extention of derivative when $\\bf{x}$ is a vector.\n","\n","* Let $y = f(x_1,\\ldots,x_k)$ be a function from ${\\rm I\\!R}^k$ to ${\\rm I\\!R}$.\n","\n","* We define $\\frac{\\partial y}{\\partial x_j}$ as the **partial derivative** of $y$ w.r.t $x_j$ which corresponds to $\\frac{d}{dx_j}(y)$ where all  $x_i,i \\neq j$ are  considered constant.\n","\n","* We define $\\nabla_{\\bf{x}}$ , the gradient operator with respect to $\\bf{x} = [x_1,\\ldots,x_k]^T$ applied to $y$ as vector of all partial derivatives\n","\n","\\begin{equation}\n","\\nabla_{\\bf{x}} y =  \n","\\begin{bmatrix}\n","\\frac{\\partial y}{\\partial x_1} \\\\\n","\\vdots\\\\\n","\\frac{\\partial y}{\\partial x_k} \n","\\end{bmatrix}.\n","\\end{equation}\n","\n","* Each element $\\frac{\\partial y}{\\partial x_j}$ indicates the effect of $x_j$ on $y$ when $x_j$ increases by an  infinitesimally small quantity $h$. "]},{"cell_type":"markdown","metadata":{"id":"3VhKM8rLI0T5"},"source":["# Exercice 1"]},{"cell_type":"markdown","metadata":{"id":"IIUhL2LdI0T6"},"source":["### Problem\n","\n","* let $f(x_1,x_2) = 2x_1^2 + 4x_2^2 - 10$ be an objective function to be minimized;\n","\n","\n","* Knowing that $f(x)$ is **convex**, use the gradient to find the minimum of $f$ ?"]},{"cell_type":"markdown","metadata":{"id":"tXtxLZoII0T7"},"source":["### Solution\n","\n","* Critical point of $f(x_1,x_2)$ is determined by values of $x_1,x_2$ for which\n","\n","\\\\\n","\n","\n","$$\n","\\nabla f = \n","\\begin{bmatrix}\n","0 \\\\\n","0\n","\\end{bmatrix}, \\iff\n","\\left\\{\n","\\begin{array}{ll}\n","4x_1 = 0,\\\\\n","8x_2 = 0 ,\n","\\end{array}\n","\\right.\n","\\iff\n","\\left\\{\n","\\begin{array}{ll}\n","x_1 = 0,\\\\\n","x_2 = 0 .\n","\\end{array}\n","\\right.\n","$$"]},{"cell_type":"markdown","metadata":{"id":"N641Eu6AI0T7"},"source":["* Equating gradient to the null vector and finding $\\bf{x}$ that satisfies that equation is refered as the **analytic** approach.\n","\n","\n","* This approach becomes impractible when it comes to functions with millions of variables;\n","\n","\n","* An alternative is to use **iterative** approaches like the gradient descent algorithm."]},{"cell_type":"markdown","metadata":{"id":"UQlg_04hI0T8"},"source":["# Gradient Descent "]},{"cell_type":"markdown","metadata":{"id":"yNB4lbY8I0T9"},"source":["* Gradient Descent is a simple algorithm used to optimize multivariable functions ($f(\\bf{x})$ where $\\bf{x} = [x_1,\\ldots,x_k]^T$). \n","\n","* Consider minimization problem, gradient descent proceeds as follows:\n","\n","![](https://drive.google.com/uc?export=view&id=1CODDYsJPk41eYGWNRKJBWiEvVGAWoraN)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xsF31N6ZI0T-"},"source":["* initialize random solution $\\bf{x}^*$ (for example it may randomly start at $x^* = 6$);\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NBkQH3E5I0T_"},"source":["* update iteratively  $\\bf{x}^*$ in such way $f$ decreases  ($x^* = 4, 2, 0$ after first, second and third updates);\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mefavY_aI0UA"},"source":["* end the process after some criterion is met (number of iterations, value of $f\\ldots$).\n"]},{"cell_type":"markdown","metadata":{"id":"XdVhO0OhI0UB"},"source":["* Gradient Descent requires two mecanisms \n","    1.  $\\lambda > 0$ refered as **convergence rate** regulating the length of step by which the solution moves.\n","    2. Stopping Condition:\n","    \n","      *  Oftentimes number of iterations $N$;\n","      \n","      * Another way is to set threshold $\\delta \\in {\\rm I\\!R}$ such that $f(\\bf{x}^*) <= \\delta$.\n","      \n","      * The two stopping criterions can be combined.\n","    \n","* Update operation is peformed on each element $x_j^{*[i]}$ as follows\n","\n","\\begin{equation}\n","x_{j}^{*[i]} \\leftarrow x_{j}^{*[i-1]} - \\lambda \\frac{\\partial f}{\\partial x_{j}^{*[i-1]}}, \n","\\end{equation}\n","\n","where $x_{j}^{*[i]}$ is the value of $x_{j}^{*}$ at $i^{th}$ iteration."]},{"cell_type":"markdown","metadata":{"id":"KucDbhkDI0UC"},"source":["* Gradient descent algorithm can be formally summarized as:\n","       \n"," * **Initialize Solution**: \n","     \\begin{equation}\n","     \\bf{x}^{*[0]} \\leftarrow \\mbox{Random()};\n","     \\end{equation}\n","     \n"," * **For** $i = 1,\\ldots,N$:\n","     \n","      * **Compute Gradient** :\n","     \n","        \\begin{equation}\n","      \\nabla_{\\bf{x}^{[i]}} f =  \n","      \\begin{bmatrix}\n","      \\frac{\\partial f}{\\partial x_1^{[i]}}\\\\\n","      \\vdots\\\\\n","      \\frac{\\partial f}{\\partial x_k^{[i]}}\n","      \\end{bmatrix};\n","      \\end{equation}   \n","      \n","      * **Update Solution**:        \n","         \\begin{equation}\n","         \\bf{x}^{*[i]} \\leftarrow  \\bf{x}^{*[i-1]} - \\lambda \\nabla_{\\bf{x}^{[i]}} f;\n","        \\end{equation}  "]}]}