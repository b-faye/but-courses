{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Tabular data"
      ],
      "metadata": {
        "id": "cRSGN4BcgBUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREDICT THE TEMPERATURE**<br>\n",
        "\n",
        "1. Load the dataset weatherHistory_bis.csv using pandas<br>\n",
        "\n",
        "2. Use seaborn or matplotlib to visualize the relationship between variables and the target Temperature.<br>\n",
        "\n",
        "3. Normalize the data using MinMaxScaler<br>\n",
        "\n",
        "4. Split the dataset into train and test sets<br>\n",
        "\n",
        "5. Create your own model to predict the Temperature using mean_square_error for loss and adam for optimizer (make sure to save history in fit)\n",
        "\n",
        "6. Visualize the curve of loss and val_loss\n",
        "\n",
        "7. Evaluate the model on the test dataset using loss (mse) and $r^2$ (determination coefficient).\n",
        "\n",
        "8. Visualize the difference between Temperature predictions and right tenmperatures on the test dataset\n",
        "\n",
        "9. Create a function to predict the temperature on a given features"
      ],
      "metadata": {
        "id": "b-yddalzgd76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Movie review classification"
      ],
      "metadata": {
        "id": "h8_HY9PEgHT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Movie review classification**\n",
        "\n",
        "In this exercise, we will use a dataset in tensorflow-datasets (tfds) and a pretrained model to encode our texts in mdb_reviews.<br>\n",
        "\n",
        "1. Import the necessary packages (tensorflow, tensorflow-datasets and tensorflow_hubs) <br>\n",
        "\n",
        "2. Load the dataset mdb_reviews with tfds\n",
        "\n",
        "3. print the 10 first reviews on the dataset\n",
        "\n",
        "4. Load the pretrained model with hub on https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1 (don't download the model)\n",
        "\n",
        "5. Create a model for predicting the review text by using the pretrained model as Input layer and add other layers after.\n",
        "\n",
        "7. Evaluate the model on test_data\n",
        "\n",
        "6. Give some reviews (texts) and evaluate your model"
      ],
      "metadata": {
        "id": "PH-4XJWAjWtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import the necessary packages (tensorflow, tensorflow-datasets and tensorflow_hubs)\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds # to use tensorflow prebuilt datasets\n",
        "import tensorflow_hub as hub # Contains presaved models for transfer learning"
      ],
      "metadata": {
        "id": "_shrsjzpgMxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load the dataset mdb_reviews with tfds\n",
        "train_data, validation_data, test_data = tfds.load(name=\"imdb_reviews\",split=('train[:60%]', 'train[60%:]', 'test'), as_supervised=True)"
      ],
      "metadata": {
        "id": "KvxGGQy5lSxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. the 10 first reviews:\n",
        "# Since our result dataset are not of type list or dictionary we will get data as below\n",
        "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
        "print('train_examples_batch= %s' % (train_examples_batch))\n",
        "print('train_labels_batch= %s' % (train_labels_batch))"
      ],
      "metadata": {
        "id": "CDaD9EF7llAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Load the pretrained model\n",
        "pretrained_model = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
        "hub_layer = hub.KerasLayer(pretrained_model, input_shape=[], dtype=tf.string, trainable=True)\n",
        "# hub_layer is or input layer which converts text inti vectors"
      ],
      "metadata": {
        "id": "9EQrrYZrlxFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correction Controle Machine Learning"
      ],
      "metadata": {
        "id": "xsqxMGklosAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Load the Wine Quality dataset\n",
        "data = load_wine()\n",
        "wine_df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "wine_df['target'] = data.target\n",
        "\n",
        "# Task 1: Data Loading and Exploration\n",
        "# Explore the dataset\n",
        "print(wine_df.head())\n",
        "print(wine_df.describe())\n",
        "\n",
        "# Task 2: Supervised Learning (Classification)\n",
        "# Split the data into features and target variable\n",
        "X = wine_df.drop('target', axis=1)\n",
        "y = wine_df['target']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Choose a classification model (Random Forest) and train it\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Task 3: Unsupervised Learning (Dimensionality Reduction)\n",
        "# Perform Principal Component Analysis (PCA)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Visualize the explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
        "plt.xlabel('Principal Components')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.show()\n",
        "\n",
        "# Task 4: Unsupervised Learning (Clustering)\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "wine_df['cluster'] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Evaluate clustering using silhouette score\n",
        "silhouette_avg = silhouette_score(X_scaled, wine_df['cluster'])\n",
        "print(f\"Silhouette Score: {silhouette_avg:.2f}\")\n",
        "\n",
        "# Task 5: Combined Supervised and Unsupervised Learning\n",
        "# Add cluster labels as an additional feature to X\n",
        "X['cluster'] = wine_df['cluster']\n",
        "\n",
        "# Re-train the classification model with the added feature\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy (with cluster feature): {accuracy:.2f}\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "dsq3RymZoyRM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}