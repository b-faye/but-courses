{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "UWxItx1WAJKU",
        "x3pTdYVDyBOa",
        "uE42jFtQyBOa",
        "v4aN25BHQovr",
        "EqrUlGcODn-_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Costum Chatbot**"
      ],
      "metadata": {
        "id": "UWxItx1WAJKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Packages**"
      ],
      "metadata": {
        "id": "CUrAVNgOARJs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSQnesCW__Pe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Dense\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import unicodedata\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load datasets**"
      ],
      "metadata": {
        "id": "xak1edrIAtMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question  = []\n",
        "answer = []\n",
        "with open(\"dialogs.txt\",'r') as f :\n",
        "    for line in f :\n",
        "        line  =  line.split('\\t')\n",
        "        question.append(line[0])\n",
        "        # use strip for removing '\\n'\n",
        "        answer.append(line[1].strip())"
      ],
      "metadata": {
        "id": "-Oh8xWzZAvpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46ctjVXlA5Dm",
        "outputId": "0c7aebb3-2aa8-47e6-b2ec-4f75c41a0e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3725"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezQojfK-A6Zk",
        "outputId": "13e850a5-1c10-4c85-bcd7-3a9756025e12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3725"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6GFUeMwA90D",
        "outputId": "0fc7ac10-e16c-4f8e-db7e-1450557d365b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi, how are you doing?',\n",
              " \"i'm fine. how about yourself?\",\n",
              " \"i'm pretty good. thanks for asking.\",\n",
              " 'no problem. so how have you been?',\n",
              " \"i've been great. what about you?\",\n",
              " \"i've been good. i'm in school right now.\",\n",
              " 'what school do you go to?',\n",
              " 'i go to pcc.',\n",
              " 'do you like it there?',\n",
              " \"it's okay. it's a really big campus.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pG6bOJ0QBAkm",
        "outputId": "c8ddab40-ab0a-4a3e-f223-ca03c7e1433b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"i'm fine. how about yourself?\",\n",
              " \"i'm pretty good. thanks for asking.\",\n",
              " 'no problem. so how have you been?',\n",
              " \"i've been great. what about you?\",\n",
              " \"i've been good. i'm in school right now.\",\n",
              " 'what school do you go to?',\n",
              " 'i go to pcc.',\n",
              " 'do you like it there?',\n",
              " \"it's okay. it's a really big campus.\",\n",
              " 'good luck with school.']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transform data to pandas dataframe**"
      ],
      "metadata": {
        "id": "aWZwIb12BeCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame({\"question\" : question ,\"answer\":answer})\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "M0g-psz2Bi-Z",
        "outputId": "d336793d-cdee-4a02-8236-88d52fb24c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                              question  \\\n",
              "0               hi, how are you doing?   \n",
              "1        i'm fine. how about yourself?   \n",
              "2  i'm pretty good. thanks for asking.   \n",
              "3    no problem. so how have you been?   \n",
              "4     i've been great. what about you?   \n",
              "\n",
              "                                     answer  \n",
              "0             i'm fine. how about yourself?  \n",
              "1       i'm pretty good. thanks for asking.  \n",
              "2         no problem. so how have you been?  \n",
              "3          i've been great. what about you?  \n",
              "4  i've been good. i'm in school right now.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b7544d5b-d28a-49e1-9d56-283242b4d1d8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hi, how are you doing?</td>\n",
              "      <td>i'm fine. how about yourself?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i'm fine. how about yourself?</td>\n",
              "      <td>i'm pretty good. thanks for asking.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i'm pretty good. thanks for asking.</td>\n",
              "      <td>no problem. so how have you been?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>no problem. so how have you been?</td>\n",
              "      <td>i've been great. what about you?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i've been great. what about you?</td>\n",
              "      <td>i've been good. i'm in school right now.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b7544d5b-d28a-49e1-9d56-283242b4d1d8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b7544d5b-d28a-49e1-9d56-283242b4d1d8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b7544d5b-d28a-49e1-9d56-283242b4d1d8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5cde87de-9009-4fb3-b5a2-0186d3533d18\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5cde87de-9009-4fb3-b5a2-0186d3533d18')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5cde87de-9009-4fb3-b5a2-0186d3533d18 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pre-processing the dataset**"
      ],
      "metadata": {
        "id": "JhFm8MIUCMFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unicode_to_ascii(s):\n",
        "    # Define a function called 'unicode_to_ascii' that takes a Unicode string 's' as input.\n",
        "\n",
        "    # Normalize the input string 's' in the \"NFD\" form using unicodedata.\n",
        "    normalized_string = unicodedata.normalize('NFD', s)\n",
        "\n",
        "    # Initialize an empty string to store the result.\n",
        "    result = ''\n",
        "\n",
        "    # Iterate through each character 'c' in the normalized string.\n",
        "    for c in normalized_string:\n",
        "        # Check if the Unicode category of the character 'c' is not 'Mn',\n",
        "        # which means it is not a non-spacing mark (diacritic).\n",
        "        if unicodedata.category(c) != 'Mn':\n",
        "            # If the character is not a diacritic, include it in the result.\n",
        "            result += c\n",
        "\n",
        "    # Return the result string with diacritics removed.\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "Y6mlPcwkCPX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Define a function called 'clean_text' that takes a text as input.\n",
        "\n",
        "    # Convert text to lowercase, remove leading/trailing whitespace, and normalize Unicode characters.\n",
        "    text = unicode_to_ascii(text.lower().strip())\n",
        "\n",
        "    # Replace contractions with their expanded forms.\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"\\r\", \"\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "\n",
        "    # Replace contractions and abbreviations with their expanded forms.\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "\n",
        "    # Replace specific contractions.\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "\n",
        "    # Replace general contractions with \"not.\"\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "\n",
        "    # Replace an apostrophe followed by 'ng' with 'ng.'\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "\n",
        "    # Replace specific contractions with their expanded forms.\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "\n",
        "    # Remove various special characters and punctuation.\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "\n",
        "    # Remove remaining punctuation using string.punctuation.\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Replace non-word characters (e.g., symbols) with a space.\n",
        "    text = re.sub(\"(\\\\W)\", \" \", text)\n",
        "\n",
        "    # Remove words containing digits.\n",
        "    text = re.sub('\\S*\\d\\S*\\s*', '', text)\n",
        "\n",
        "    # Add start and end tokens (\"<sos>\" and \"<eos>\") to the text.\n",
        "    text = \"<sos> \" + text + \" <eos>\"\n",
        "\n",
        "    # Return the cleaned and processed text.\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "H9-c9lGGCtcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"question\"] = data.question.apply(clean_text)\n",
        "data[\"question\"].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E7N3hH0DDA5",
        "outputId": "dae7ba11-23a4-4fea-d91b-0c0ef7fbe3b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                  <sos> hi how are you doing <eos>\n",
              "1          <sos> i am fine how about yourself <eos>\n",
              "2    <sos> i am pretty good thanks for asking <eos>\n",
              "3       <sos> no problem so how have you been <eos>\n",
              "4      <sos> i have been great what about you <eos>\n",
              "Name: question, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"answer\"] = data.answer.apply(clean_text)\n",
        "data[\"answer\"].head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCJOZ6BlDLAt",
        "outputId": "1a763a58-3188-40a4-cb3f-24c1225fa891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0             <sos> i am fine how about yourself <eos>\n",
              "1       <sos> i am pretty good thanks for asking <eos>\n",
              "2          <sos> no problem so how have you been <eos>\n",
              "3         <sos> i have been great what about you <eos>\n",
              "4    <sos> i have been good i am in school right no...\n",
              "Name: answer, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question  = data.question.values.tolist()\n",
        "answer =  data.answer.values.tolist()"
      ],
      "metadata": {
        "id": "nxT8zcM5DUPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenize the data**"
      ],
      "metadata": {
        "id": "2cvNPszADkrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(lang):\n",
        "    # Define a function called 'tokenize' that takes a 'lang' as input (a list of sentences).\n",
        "\n",
        "    # Create a Tokenizer with no filters (no filtering of characters).\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "\n",
        "    # Fit the Tokenizer on the provided 'lang' data to create a vocabulary.\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    # Convert the sentences in 'lang' to sequences of integers using the Tokenizer.\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    # Pad the sequences with zeros to make them of equal length (post-padding).\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    # Return the padded sequences (tensors) and the language Tokenizer.\n",
        "    return tensor, lang_tokenizer"
      ],
      "metadata": {
        "id": "QErpCh_zDnzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor , inp_lang  =  tokenize(question)"
      ],
      "metadata": {
        "id": "hBry3PjBDzB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_tensor , targ_lang  =  tokenize(answer)"
      ],
      "metadata": {
        "id": "fqgTt1N4D2VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "metadata": {
        "id": "IUBVjzVbEPo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length_targ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9QOoKArEQ03",
        "outputId": "906e799f-5972-4310-decf-25ce87780922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length_inp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVDXyffWES7f",
        "outputId": "663ba38c-2fb9-47ab-803a-608347e6b2bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset for train and test\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
      ],
      "metadata": {
        "id": "KPTXW53LEaok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Make batch**"
      ],
      "metadata": {
        "id": "twjSVVY4E3Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)  # Define the buffer size, typically the number of training examples.\n",
        "BATCH_SIZE = 64  # Define the batch size for training data.\n",
        "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE  # Calculate the number of steps per training epoch.\n",
        "embedding_dim = 256  # Define the dimension of word embeddings.\n",
        "units = 1024  # Define the number of units or neurons in a recurrent neural network (RNN) layer.\n",
        "vocab_inp_size = len(inp_lang.word_index) + 1  # Calculate the size of the input vocabulary.\n",
        "vocab_tar_size = len(targ_lang.word_index) + 1  # Calculate the size of the target vocabulary.\n",
        "\n",
        "# Create a TensorFlow dataset from the input and target tensors, and shuffle it using the specified BUFFER_SIZE.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "\n",
        "# Batch the dataset into batches of BATCH_SIZE and drop any remaining examples that don't fit into a batch.\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Get an example input batch and an example target batch from the dataset.\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "\n",
        "# Print the shapes of the example input and target batches.\n",
        "print(\"Example Input Batch Shape:\", example_input_batch.shape)\n",
        "print(\"Example Target Batch Shape:\", example_target_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcgjpa_kE5u0",
        "outputId": "96df83de-74fa-4639-dc8c-1255e186023a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Input Batch Shape: (64, 22)\n",
            "Example Target Batch Shape: (64, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create our Encoder**"
      ],
      "metadata": {
        "id": "rATqq2uZFMdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom class called \"Encoder\" that inherits from the tf.keras.Model class.\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()  # Call the constructor of the parent class.\n",
        "        self.batch_sz = batch_sz  # Store the batch size as an instance variable.\n",
        "        self.enc_units = enc_units  # Store the number of units in the GRU layer.\n",
        "\n",
        "        # Create an embedding layer to convert input tokens into dense vectors.\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Create a GRU (Gated Recurrent Unit) layer with specified parameters.\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # Define the forward pass for the encoder.\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)  # Pass the input through the embedding layer.\n",
        "        output, state = self.gru(x, initial_state=hidden)  # Pass through the GRU.\n",
        "        return output, state  # Return the output sequence and final hidden state.\n",
        "\n",
        "    # Initialize the hidden state (typically with zeros).\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))\n"
      ],
      "metadata": {
        "id": "5PF3VNVFFPaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-SYp0b7FuNG",
        "outputId": "8616f173-8268-40ea-ca41-8c1aefa37409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 22, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create attention layer**"
      ],
      "metadata": {
        "id": "oPh2yFrGIldh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # query hidden state shape == (batch_size, hidden size)\n",
        "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # values shape == (batch_size, max_len, hidden size)\n",
        "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "_EBrBe3NJWsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdOGStAmJcb4",
        "outputId": "ebff117e-71e4-4fb6-acd2-88b447c2866d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 22, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create Decoder**"
      ],
      "metadata": {
        "id": "iieJ2QtWKWYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # used for attention\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights"
      ],
      "metadata": {
        "id": "tWrc3BJVKZpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBD_xvE8KlNi",
        "outputId": "6aa78103-a0e1-4eee-9341-a9ac7d900143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 2347)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create optimizer**"
      ],
      "metadata": {
        "id": "A7YJMCPPKvHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "metadata": {
        "id": "8k8KfFMiKyP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training function**"
      ],
      "metadata": {
        "id": "l-ZdQYcSLGcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims([targ_lang.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss"
      ],
      "metadata": {
        "id": "jTTef52fLIh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train the model**"
      ],
      "metadata": {
        "id": "6v-TVopxLQZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 40\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "    if(epoch % 4 == 0):\n",
        "        print('Epoch:{:3d} Loss:{:.4f}'.format(epoch,\n",
        "                                          total_loss / steps_per_epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW4kNI9KLSYj",
        "outputId": "93c33592-194d-4e9f-8824-6c59f0499454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  4 Loss:1.5763\n",
            "Epoch:  8 Loss:1.3437\n",
            "Epoch: 12 Loss:1.1768\n",
            "Epoch: 16 Loss:1.0126\n",
            "Epoch: 20 Loss:0.8451\n",
            "Epoch: 24 Loss:0.6584\n",
            "Epoch: 28 Loss:0.4661\n",
            "Epoch: 32 Loss:0.2697\n",
            "Epoch: 36 Loss:0.1305\n",
            "Epoch: 40 Loss:0.0562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluate the model**"
      ],
      "metadata": {
        "id": "sZVf6T5flGIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_tags(sentence):\n",
        "    return sentence.split(\"<start>\")[-1].split(\"<end>\")[0]"
      ],
      "metadata": {
        "id": "aQRZBpV2lmLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence):\n",
        "    sentence = clean_text(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<sos>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<eos>':\n",
        "            return remove_tags(result), remove_tags(sentence)\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return remove_tags(result), remove_tags(sentence)"
      ],
      "metadata": {
        "id": "QsX6QgjVlJo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions  = []\n",
        "answers = []\n",
        "with open(\"dialogs.txt\",'r') as f :\n",
        "    for line in f :\n",
        "        line  =  line.split('\\t')\n",
        "        questions.append(line[0])\n",
        "        answers.append(line[1].strip())"
      ],
      "metadata": {
        "id": "8n72xUV7lLAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "\n",
        "    print('Question: %s' % (sentence))\n",
        "    print('Predicted answer: {}'.format(result))"
      ],
      "metadata": {
        "id": "Zs5PaOeCmEwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions[12]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Y8DhPSIgofPs",
        "outputId": "b1e27262-21b8-4ac0-a39f-96b7faad1f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"i'm doing well. how about you?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask(questions[12])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvPnEee7mIwY",
        "outputId": "cea5eba3-f247-4731-b59a-971283a5b855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: <sos> i am doing well how about you <eos>\n",
            "Predicted answer: never better thanks <eos> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qRMQ_13EtGQA",
        "outputId": "8b612d31-d1c3-46ed-c9f6-66047291e3c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i believe so.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask(questions[100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FycnT26tCe3",
        "outputId": "91e31031-a2ea-45bc-a17b-4c1d7dde89b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: <sos> i believe so <eos>\n",
            "Predicted answer: good i hope it does not cool off this weekend <eos> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5QO6EJtutJ6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4k8VEVjstJ8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ask('Hello how are you!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3axnbQETtJ_Y",
        "outputId": "7792ea06-556a-4129-8327-4bdea3d9093b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: <sos> hello how are you <eos>\n",
            "Predicted answer: i am so full i am going to burst <eos> \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NO1QCR-7x9VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GPT2 Text Generation**"
      ],
      "metadata": {
        "id": "WgnFIcOo-6l-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -r *"
      ],
      "metadata": {
        "id": "NSghXnGfaKDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/keras-team/keras-nlp.git -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkCuhWUI-_US",
        "outputId": "c3b1483a-1691-40a5-9f5a-97b6f9c028e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "import keras_core as keras\n",
        "import time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waBVEZDY_IS0",
        "outputId": "efdfe08a-3195-424c-f219-337506ef8651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using JAX backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=128,\n",
        ")\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    \"gpt2_base_en\", preprocessor=preprocessor\n",
        ")"
      ],
      "metadata": {
        "id": "f2Ee8HdK_bmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GPT2 Demo**"
      ],
      "metadata": {
        "id": "v4aN25BHQovr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a text\n",
        "output = gpt2_lm.generate(\"The goal of apple company\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgwbyqaL_n6X",
        "outputId": "507d454a-d107-4836-cf75-32ee9e254530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "The goal of apple company, Inc. was to make a better apple.\n",
            "\n",
            "\"I don't know how you would describe it, but the apple is the best apple we ever tasted, so we're very pleased with it,\" said Steve Hirsch, Apple's president and CEO. \"It is one of the best apples ever.\"\n",
            "\n",
            "The company is now working with the U.S. Department of Agriculture to develop and sell the apple.\n",
            "\n",
            "The company is currently testing its product on apples grown by farmers in the Midwest, and is also working with the U.S. Department of Agriculture to test its products on apples grown in the Midwest.\n",
            "\n",
            "Apple's apple has been the focus of controversy since its introduction in 2007 and its reputation has been tarnished by its poor quality.\n",
            "\n",
            "Apple's reputation has been tarnished by a series of lawsuits that it has filed against the U.S. government, which is investigating the company for fraud.\n",
            "\n",
            "Apple is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a text\n",
        "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "bgmDuYcZEvfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7293859b-aa44-4b7a-ff34-4f8934afd6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "I like basketball so much that I don't think I can be a part of it. I'm a good basketball player, and I love basketball. I'm going to be a great basketball player, so that's all I can do.\n",
            "\n",
            "I don't have to be a great basketball player to do this kind of stuff, but I'm going to get to be an awesome basketball player, because that's my job.\n",
            "\n",
            "It's been a long time coming since I was a kid. I was in the NBA at 16 and I was playing for the New Jersey Nets. And I was just a kid. I was playing against my dad, my brother. And I was just like, \"What are you doing? What is this going to be like?\" I was like, \"I'm just going to go play. I'm going to do this.\"\n",
            "\n",
            "I'm going to do that. I'm going to go out and play, and then I'll be like\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a text\n",
        "output = gpt2_lm.generate(\"What is basketball?\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yritglBCPIdR",
        "outputId": "b9ffd2f1-6a47-4199-d7d1-521807856465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "What is basketball?\n",
            "\n",
            "It is a sport that involves two teams competing in an event, usually in a series of matches, with the goal of reaching a championship game. The NBA, on the other hand, has two teams competing in an event, usually at the same time, which means that each team has its own unique game plan.\n",
            "\n",
            "The NBA's two teams have different rules for the game, but it is generally agreed that the teams are to play in the same game, but the rules are different depending on the game. The rules for basketball are:\n",
            "\n",
            "Each team has its own rules for a particular game\n",
            "\n",
            "Each team plays the same game, but has its own rules for the next game\n",
            "\n",
            "Each team plays a particular game, but has its own rules for the next game\n",
            "\n",
            "The game rules for basketball have a number of variations. For example, if the team plays a game that is called \"Finals,\" the other team will play \"Final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a text\n",
        "output = gpt2_lm.generate(\"What is football?\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uioo5VfWPS_a",
        "outputId": "f7826cf1-edb7-4e17-9b05-12d505819efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "What is football? Football is about winning, winning, winning. Winning is winning, winning, winning.\n",
            "\n",
            "The game is about winning. Winning is winning  win. Winning is winning. Winning is winning.\n",
            "\n",
            "It's not a game of football. Football is a game of winning. Winning is winning. Winning is winning, win. Win.\n",
            "\n",
            "The game is about winning. Winning is win, win. Winning is winning, win.\n",
            "\n",
            "And so we are.\n",
            "\n",
            "We are winning. We are winning. We are winning.\n",
            "\n",
            "We are not just winning  we are winning. We are winning. We are winning.\n",
            "\n",
            "We are winning. We are winning. We are winning.\n",
            "\n",
            "We are winning. We are winning.\n",
            "\n",
            "We are winning. We are winning.\n",
            "\n",
            "We are winning. We are winning.\n",
            "\n",
            "It's not a game of football. Football is a game of winning. Winning is winning  win\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a text\n",
        "output = gpt2_lm.generate(\"What is Machine Learning?\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6F9gaOgPZ8Z",
        "outputId": "7b0f4dbd-95c6-41fd-9fca-55932411657c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "What is Machine Learning?\n",
            "\n",
            "Machine learning is one of the key concepts in Artificial Intelligence. It is used to solve problems in a way that is not possible with conventional AI. It is also one of the most widely used concepts in Artificial Intelligence. Machine Learning is a set of concepts which are often used interchangeably by many different companies. Machine learning is an approach to solving problems in a way that is not possible with conventional AI. This is the key to the development and application of Machine Learning techniques.\n",
            "\n",
            "Why does Machine Learning Matter?\n",
            "\n",
            "Machine Learning is an approach to solving problems in a way that is not possible with traditional AI. It is also one of the most widely used concepts in Artificial Intelligence. Machine Learning is an approach to solving problems in a way that is not possible with traditional AI.\n",
            "\n",
            "Why Does Machine Learning Matter?\n",
            "\n",
            "Machine Learning is one of the key concepts in Artificial Intelligence. It is used to solve problems in a way that is not possible with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Fine-tune GPT2 in Reddit dataset https://www.tensorflow.org/datasets/catalog/reddit**"
      ],
      "metadata": {
        "id": "isWP9prXAiDY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you have the knowledge of the GPT-2 model from KerasNLP, you can take one step further to finetune the model so that it generates text in a specific style, short or long, strict or casual. In this tutorial, we will use reddit dataset for example."
      ],
      "metadata": {
        "id": "lcTlv7K1rvrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "reddit_ds = tfds.load(\"reddit_tifu\", split=\"train\", as_supervised=True)"
      ],
      "metadata": {
        "id": "jvjae3IqAkxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit_ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDHk8xOVCIOK",
        "outputId": "943a0096-1380-4c7c-a8a0-b7402cf18cd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for document, title in reddit_ds:\n",
        "    print(document.numpy())\n",
        "    print(title.numpy())\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWccJcP2Byz9",
        "outputId": "c31933b1-ed1b-4993-8882-27bdd3e626d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"me and a friend decided to go to the beach last sunday. we loaded up and headed out. we were about half way there when i decided that i was not leaving till i had seafood. \\n\\nnow i'm not talking about red lobster. no friends i'm talking about a low country boil. i found the restaurant and got directions. i don't know if any of you have heard about the crab shack on tybee island but let me tell you it's worth it. \\n\\nwe arrived and was seated quickly. we decided to get a seafood sampler for two and split it. the waitress bought it out on separate platters for us. the amount of food was staggering. two types of crab, shrimp, mussels, crawfish, andouille sausage, red potatoes, and corn on the cob. i managed to finish it and some of my friends crawfish and mussels. it was a day to be a fat ass. we finished paid for our food and headed to the beach. \\n\\nfunny thing about seafood. it runs through me faster than a kenyan \\n\\nwe arrived and walked around a bit. it was about 45min since we arrived at the beach when i felt a rumble from the depths of my stomach. i ignored it i didn't want my stomach to ruin our fun. i pushed down the feeling and continued. about 15min later the feeling was back and stronger than before. again i ignored it and continued. 5min later it felt like a nuclear reactor had just exploded in my stomach. i started running. i yelled to my friend to hurry the fuck up. \\n\\nrunning in sand is extremely hard if you did not know this. we got in his car and i yelled at him to floor it. my stomach was screaming and if he didn't hurry i was gonna have this baby in his car and it wasn't gonna be pretty. after a few red lights and me screaming like a woman in labor we made it to the store. \\n\\ni practically tore his car door open and ran inside. i ran to the bathroom opened the door and barely got my pants down before the dam burst and a flood of shit poured from my ass. \\n\\ni finished up when i felt something wet on my ass. i rubbed it thinking it was back splash. no, mass was covered in the after math of me abusing the toilet. i grabbed all the paper towels i could and gave my self a whores bath right there. \\n\\ni sprayed the bathroom down with the air freshener and left. an elderly lady walked in quickly and closed the door. i was just about to walk away when i heard gag. instead of walking i ran. i got to the car and told him to get the hell out of there.\"\n",
            "b'liking seafood'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = (\n",
        "    reddit_ds.map(lambda document, _: document)\n",
        "    .batch(32)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "jYQyGXKoCQTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = train_ds.take(500)\n",
        "num_epochs = 1\n",
        "\n",
        "# Linearly decaying learning rate.\n",
        "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
        "    5e-5,\n",
        "    decay_steps=train_ds.cardinality() * num_epochs,\n",
        "    end_learning_rate=0.0,\n",
        ")\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "gpt2_lm.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "gpt2_lm.fit(train_ds, epochs=num_epochs)"
      ],
      "metadata": {
        "id": "wGL-sixsCU0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc7dd54e-5ac6-4ffe-f7be-2c988220b8f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m500/500\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m578s\u001b[0m 1s/step - accuracy: 0.3190 - loss: 3.3643\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_core.src.callbacks.history.History at 0x7d80b4abb790>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = gpt2_lm.generate(\"What is basketball?\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "rXO7m1YWCkWw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1431d9f7-abb9-42df-9c30-95fcd77487dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "What is basketball?\n",
            "\n",
            "let's start with the basics. basketball is a basketball game where you play the ball, which means that you can shoot it, and you can shoot it. \n",
            "\n",
            "you have to score a basket, and you can't shoot the ball, so you're basically shooting the ball. \n",
            "\n",
            "you shoot it, you shoot it and you get the basket, and you score a free throw, and then you get the basket back, and then you get the basket back, and you get the ball back, and you get the free throw back, and then you get the free throw back and you get the basket back, and then you get the free throw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SEARCH FOR TOP-K\n",
        "# Use a string identifier.\n",
        "gpt2_lm.compile(sampler=\"top_k\")\n",
        "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)\n",
        "\n",
        "# Use a `Sampler` instance. `GreedySampler` tends to repeat itself,\n",
        "greedy_sampler = keras_nlp.samplers.GreedySampler()\n",
        "gpt2_lm.compile(sampler=greedy_sampler)\n",
        "\n",
        "output = gpt2_lm.generate(\"I like basketball\", max_length=200)\n",
        "print(\"\\nGPT-2 output:\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "4mvfao_UC8TC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38c3b3c7-cedc-4567-eaae-6b824d0dacaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPT-2 output:\n",
            "I like basketball and it's a great sport and it's a great opportunity for me to be part of the team and be involved in the program.\n",
            "\n",
            "the other day a guy was talking about how he wanted to play a basketball game with his family. he had a friend who was in the team and wanted to play a basketball game with his family so he asked the friend for a game.\n",
            "\n",
            "\"what's your favorite basketball game?\"\n",
            "\n",
            "\"that's what i play.\"\n",
            "\n",
            "\"you're not a bad person!\"\n",
            "\n",
            "\"you're not.\"\n",
            "\n",
            "\"i don't play basketball.\"\n",
            "\n",
            "\"well\n",
            "\n",
            "GPT-2 output:\n",
            "I like basketball, but i don't like to play it. \n",
            "\n",
            "so i was playing basketball at my local high school, and i was playing with my friends. \n",
            "\n",
            "i was playing with my friends, and one of them was a girl. \n",
            "\n",
            "she was a girl, and she was a pretty good basketball player. \n",
            "\n",
            "so i was playing with my friends, and one of them was a girl. \n",
            "\n",
            "she was a girl, and she was a pretty good basketball player. \n",
            "\n",
            "so i was playing with my friends, and one of them was a girl. \n",
            "\n",
            "so i was playing with my friends, and one of them was a girl. \n",
            "\n",
            "so i was playing with my friends, and one of them was a girl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Don't forget to save the fine-tuned model for future tests**"
      ],
      "metadata": {
        "id": "iLp7T0AFwEIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Finetune on Chinese Poem Dataset**"
      ],
      "metadata": {
        "id": "EqrUlGcODn-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=128,\n",
        ")\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\n",
        "    \"gpt2_base_en\", preprocessor=preprocessor\n",
        ")"
      ],
      "metadata": {
        "id": "CsTpKO1S-fIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/chinese-poetry/chinese-poetry.git"
      ],
      "metadata": {
        "id": "tqCf2RArDciN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "poem_collection = []\n",
        "for file in os.listdir(\"chinese-poetry/\"):\n",
        "    if \".json\" not in file or \"poet\" not in file:\n",
        "        continue\n",
        "    full_filename = \"%s/%s\" % (\"chinese-poetry/\", file)\n",
        "    with open(full_filename, \"r\") as f:\n",
        "        content = json.load(f)\n",
        "        poem_collection.extend(content)\n",
        "\n",
        "paragraphs = [\"\".join(data[\"paragraphs\"]) for data in poem_collection]"
      ],
      "metadata": {
        "id": "1jCeYrXTDy2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(paragraphs[0])"
      ],
      "metadata": {
        "id": "BxCPWGhGD12K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(paragraphs)\n",
        "    .batch(16)\n",
        "    .cache()\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Running through the whole dataset takes long, only take `500` and run 1\n",
        "# epochs for demo purposes.\n",
        "train_ds = train_ds.take(500)\n",
        "num_epochs = 1\n",
        "\n",
        "learning_rate = keras.optimizers.schedules.PolynomialDecay(\n",
        "    5e-4,\n",
        "    decay_steps=train_ds.cardinality() * num_epochs,\n",
        "    end_learning_rate=0.0,\n",
        ")\n",
        "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "gpt2_lm.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate),\n",
        "    loss=loss,\n",
        "    weighted_metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "gpt2_lm.fit(train_ds, epochs=num_epochs)"
      ],
      "metadata": {
        "id": "mOzOroA3D5qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = gpt2_lm.generate(\"\", max_length=200)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "vfoJGYQwEAd9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}